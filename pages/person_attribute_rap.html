<!DOCTYPE html>
<html lang="en">
<head>
<title> An introduction to the RAP dataset </title>
<style>
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p,
blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em,
font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt,
dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot,
thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p,
blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em,
font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt,
dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot,
thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1000px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h4 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong, b {
    font-weight:bold;
}

em, i {
    font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  /* padding-left: 230px; */
  padding-right: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: right;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
 rel="stylesheet" type="text/css" />
</head>

<body>
<div class="section">
    <h2 style="text-align: center;">  A Richly Annotated Dataset for Pedestrian Attribute Recognition</h2>
    <br />
    <p style="text-align: center"> 
        Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang </p> <br />
</div>
    
<div class="section">
<h2> Abstract </h2>
    <div class="paper">
    In this paper, we aim to improve the dataset foundation for pedestrian attribute recognition in real surveillance scenarios. 
    Recognition of human attributes, such as gender, and clothes types, has great prospects in real applications. 
    However, the development of suitable benchmark datasets for attribute recognition remains lagged behind. 
    Existing human attribute datasets are collected from various sources or an integration of pedestrian re-identification datasets. 
    Such heterogeneous collection poses a big challenge on developing high quality fine-grained attribute recognition algorithms. 
    Furthermore, human attribute recognition are generally severely affected by environmental or contextual factors, 
    such as viewpoints, occlusions and body parts, while existing attribute datasets barely care about them. 
    To tackle these problems, we build a Richly Annotated Pedestrian (RAP) dataset from real multi-camera surveillance 
    scenarios with long term collection, where data samples are annotated with not only fine-grained human attributes 
    but also environmental and contextual factors. RAP has in total 41,585 pedestrian samples, 
    each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information. 
    To our knowledge, the RAP dataset is the largest pedestrian attribute dataset, 
    which is expected to greatly promote the study of large-scale attribute recognition systems. 
    Furthermore, we empirically analyze the effects of different environmental and contextual factors on pedestrian attribute recognition.
    Experimental results demonstrate that viewpoints, occlusions and body parts information could assist attribute 
    recognition a lot in real applications.
    </div>
</div>


<div class="section">
<h2> Dataset Information </h2>
    <div class="paper"> 
    The basic annotation information on RAP dataset can be seen from the table below. <br />
    <img title="annotation" style="float: center; height: 250px;" src="./../misc/images/rap_annotation_structure.jpg" />
    </div>
</div>


<div class="section">
<h2> Citation </h2>
    <div class="paper">
    <span> <font color="red"> To be updated </font>  </span>
    </div>
</div>


<div class="section">
<h2> Download </h2>
    <div class="paper">
    <span> <font color="red"> To be updated </font>  </span>
    </div>
</div>

</body>

</html>
